{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    " * best parameters:  'n_estimators': 700, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_depth': 18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:11:22.637854Z",
     "start_time": "2020-12-12T20:11:22.630865Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and drop features, manage categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T19:42:21.303148Z",
     "start_time": "2020-12-12T19:42:21.286296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to prepare data\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def create_new_features(X):\n",
    "    # transform utc offset:\n",
    "    X['Sin_UTC'] = np.sin((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    X['Cos_UTC'] = np.cos((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    \n",
    "    # time since creation in days\n",
    "    duration = (pd.to_datetime('today') - \n",
    "                pd.to_datetime(X['Profile Creation Timestamp']).dt.tz_localize(None))\n",
    "    X['Duration'] = duration.apply(lambda x: x.days)\n",
    "    \n",
    "    # convert personal url into True/False (NaNs or unique)\n",
    "    X['Has Personal URL'] = X['Personal URL'].notna()\n",
    "    \n",
    "    # log(x + 1) of numerical features\n",
    "    for feature in ['Num of Followers', 'Num of People Following',\n",
    "                    'Num of Status Updates', 'Num of Direct Messages',\n",
    "                    'Avg Daily Profile Visit Duration in seconds',\n",
    "                    'Avg Daily Profile Clicks']:\n",
    "        X[f'log {feature}'] = np.log1p(X[feature])\n",
    "    return X\n",
    "    \n",
    "def clean_features(X):\n",
    "    # merge categories names with and without cap letter\n",
    "    X['Location Public Visibility'] = X['Location Public Visibility'].str.lower()\n",
    "    X.loc[X['Profile Category'] == \" \", 'Profile Category'] = 'unknown'\n",
    "    return X\n",
    "\n",
    "def drop_features(X, l_features):\n",
    "    return X.drop(columns=l_features)\n",
    "    \n",
    "class RemoveCategories(TransformerMixin):\n",
    "    def __init__(self, min_obs=10, l_cols=[]):\n",
    "        self.min_obs = min_obs\n",
    "        self.l_cats = {}\n",
    "        self.l_cols = l_cols\n",
    "\n",
    "    def fit(self, X):\n",
    "        # assumes all columns of df_cat are strings\n",
    "        for col in self.l_cols:\n",
    "            val_counts = X[col].fillna('missing').value_counts()\n",
    "            self.l_cats[col] = val_counts[val_counts >= self.min_obs].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        for col in self.l_cols:\n",
    "            X.loc[~ X[col].isin(self.l_cats[col]), col] = 'other'        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T19:42:21.315210Z",
     "start_time": "2020-12-12T19:42:21.307158Z"
    }
   },
   "outputs": [],
   "source": [
    "# data cleaning and feature engineering\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "l_cat_small = ['Profile Text Color', 'Profile Page Color',\n",
    "               'Profile Theme Color', 'User Language', 'Location']\n",
    "l_drop = ['Id', 'User Name', 'Profile Image', 'Personal URL',\n",
    "          'Profile Creation Timestamp', 'Location', 'UTC Offset']\n",
    "l_drop_logs = ['Num of Followers', 'Num of People Following',\n",
    "               'Num of Status Updates', 'Num of Direct Messages',\n",
    "               'Avg Daily Profile Visit Duration in seconds', \n",
    "               'Avg Daily Profile Clicks']\n",
    "\n",
    "features_to_drop =dict(l_features = l_drop + l_drop_logs)\n",
    "\n",
    "pipe = Pipeline([('create new features',\n",
    "                  FunctionTransformer(create_new_features)),\n",
    "                 ('clean data', FunctionTransformer(clean_features)),\n",
    "                 ('remove categories', RemoveCategories(min_obs=20, \n",
    "                                                        l_cols=l_cat_small)),\n",
    "                 ('drop variables', FunctionTransformer(drop_features,\n",
    "                                    kw_args=features_to_drop))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and encoding/scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T19:42:21.337241Z",
     "start_time": "2020-12-12T19:42:21.317175Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "# categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T19:42:22.664090Z",
     "start_time": "2020-12-12T19:42:21.340557Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "y = np.log1p(df['Num of Profile Likes'].values)\n",
    "X = df.drop(columns='Num of Profile Likes')\n",
    "\n",
    "X = pipe.fit_transform(X)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer,\n",
    "                   X.select_dtypes(exclude=['object', 'bool']).columns),\n",
    "                  ('cat', categorical_transformer,\n",
    "                   X.select_dtypes(include=['object', 'bool']).columns)])\n",
    "\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:13:06.993920Z",
     "start_time": "2020-12-12T20:11:40.910985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/conda/pyanni/envs/kaggle_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE: 1.737 \n",
      "best parameters: {'n_estimators': 700, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_depth': 18}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(criterion='mse', n_jobs=-1)\n",
    "\n",
    "parameters = {'n_estimators': [700],\n",
    "              'max_depth': range(18, 19),\n",
    "              'min_samples_split': range(3, 4),\n",
    "              'min_samples_leaf': range(3, 4)}\n",
    "\n",
    "reg_rf = RandomizedSearchCV(rf, parameters, scoring='neg_mean_squared_error',\n",
    "                      n_jobs=-1, random_state=0, n_iter=20)\n",
    "reg_rf.fit(X, y)\n",
    "\n",
    "print(f'RMSLE: {np.sqrt(-reg_rf.best_score_):.3f}',\n",
    "      f'\\nbest parameters: { reg_rf.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE: 1.742 \n",
      "best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 16}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(criterion='mse', n_jobs=-1)\n",
    "\n",
    "parameters = {'n_estimators': [100, 300, 500],\n",
    "              'max_depth': range(2, 20, 2),\n",
    "              'min_samples_split': range(2, 20, 2),\n",
    "              'min_samples_leaf': range(2, 20, 2)}\n",
    "\n",
    "reg_rf = RandomizedSearchCV(rf, parameters, scoring='neg_mean_squared_error',\n",
    "                      n_jobs=-1, random_state=0, n_iter=20)\n",
    "reg_rf.fit(X, y)\n",
    "\n",
    "print(f'RMSLE: {np.sqrt(-reg_rf.best_score_):.3f}',\n",
    "      f'\\nbest parameters: { reg_rf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:05:29.006704Z",
     "start_time": "2020-12-12T20:05:28.999748Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_submission(estimator, number):\n",
    "    X_test = pd.read_csv('data/test.csv')\n",
    "    Id = X_test['Id']\n",
    "    X_test = pipe.transform(X_test)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    y_pred = (estimator.predict(X_test))\n",
    "    y_pred_test = pd.Series(np.expm1(y_pred))\n",
    "    submission = pd.DataFrame({'Id': Id, 'Predicted': y_pred_test})\n",
    "    path = f'submissions/submission{number}.csv'\n",
    "    while os.path.exists(path):\n",
    "        number += 1\n",
    "        path = f'submissions/submission{number}.csv'\n",
    "    submission.to_csv(path, index=False)\n",
    "    print(f'submission #{number} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:13:07.585547Z",
     "start_time": "2020-12-12T20:13:06.997038Z"
    }
   },
   "outputs": [],
   "source": [
    "create_submission(reg_rf, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
