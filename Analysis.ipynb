{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:31:36.914835Z",
     "start_time": "2020-12-04T04:31:35.702338Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:31:36.974948Z",
     "start_time": "2020-12-04T04:31:36.916778Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "y = np.log1p(df['Num of Profile Likes'].values)\n",
    "X = df.drop(columns='Num of Profile Likes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and drop features, manage categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:31:37.192057Z",
     "start_time": "2020-12-04T04:31:36.977953Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def create_new_features(X):\n",
    "    # transform utc offset:\n",
    "    X['Sin_UTC'] = np.sin((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    X['Cos_UTC'] = np.cos((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    \n",
    "    # time since creation in days\n",
    "    duration = (pd.to_datetime('today') - \n",
    "                pd.to_datetime(X['Profile Creation Timestamp']).dt.tz_localize(None))\n",
    "    X['Duration'] = duration.apply(lambda x: x.days)\n",
    "    \n",
    "    # convert personal url into True/False (NaNs or unique)\n",
    "    X['Has Personal URL'] = X['Personal URL'].notna()\n",
    "    \n",
    "    # log(x + 1) of numerical features\n",
    "    for feature in ['Num of Followers', 'Num of People Following',\n",
    "                    'Num of Status Updates', 'Num of Direct Messages',\n",
    "                    'Avg Daily Profile Visit Duration in seconds',\n",
    "                    'Avg Daily Profile Clicks']:\n",
    "        X[f'log {feature}'] = np.log1p(X[feature])\n",
    "\n",
    "    return X\n",
    "    \n",
    "def clean_features(X):\n",
    "    # merge categories names with and without cap letter\n",
    "    X['Location Public Visibility'] = X['Location Public Visibility'].str.lower()\n",
    "    return X\n",
    "\n",
    "def drop_features(X, l_features):\n",
    "    return X.drop(columns=l_features)\n",
    "    \n",
    "class RemoveCategories(TransformerMixin):\n",
    "    def __init__(self, min_obs=10, l_cols=[]):\n",
    "        self.min_obs = min_obs\n",
    "        self.l_cats = {}\n",
    "        self.l_cols = l_cols\n",
    "\n",
    "    def fit(self, X):\n",
    "        # assumes all columns of df_cat are strings\n",
    "        for col in self.l_cols:\n",
    "            val_counts = X[col].fillna('missing').value_counts()\n",
    "            self.l_cats[col] = val_counts[val_counts >= self.min_obs].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        for col in self.l_cols:\n",
    "            X.loc[~ X[col].isin(self.l_cats[col]), col] = 'other'        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:31:38.409618Z",
     "start_time": "2020-12-04T04:31:37.194028Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "l_cat_small = ['Profile Text Color', 'Profile Page Color',\n",
    "               'Profile Theme Color', 'User Language', 'Location',\n",
    "               'User Time Zone']\n",
    "features_to_drop =dict(l_features = ['Id', 'User Name', 'Profile Image',\n",
    "                                     'Personal URL', 'UTC Offset', \n",
    "                                     'Profile Creation Timestamp'])\n",
    "\n",
    "pipe = Pipeline([('create new features',\n",
    "                  FunctionTransformer(create_new_features)),\n",
    "                 ('clean data', FunctionTransformer(clean_features)),\n",
    "                 ('remove categories', RemoveCategories(min_obs=10,\n",
    "                                                        l_cols=l_cat_small)),\n",
    "                 ('drop variables', FunctionTransformer(drop_features,\n",
    "                                    kw_args=features_to_drop))\n",
    "                ])\n",
    "\n",
    "X = pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and encoding/scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:31:38.605359Z",
     "start_time": "2020-12-04T04:31:38.411578Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# numerical features\n",
    "num_features = X.select_dtypes(exclude=['object', 'bool']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "cat_features = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "# categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse=False))])\n",
    "\n",
    "# preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, num_features),\n",
    "                  ('cat', categorical_transformer, cat_features)])\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:40:02.823007Z",
     "start_time": "2020-12-04T04:40:01.494079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.22583512, -3.21080217, -7.00795668, -3.29166546, -3.15756606,\n",
       "       -3.05369605, -3.0835151 , -3.36914894, -3.47373171, -3.70077048])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "reg = LinearRegression()\n",
    "scores = cross_val_score(reg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:53:32.632087Z",
     "start_time": "2020-12-04T04:53:30.261494Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.307747332432129 {'alpha': 562.341325190349, 'normalize': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "parameters = {'alpha': np.logspace(-1, 4, 5), 'normalize':[False, True]}\n",
    "reg = Ridge()\n",
    "reg_cv = GridSearchCV(reg, parameters, scoring='neg_mean_squared_error')\n",
    "reg_cv.fit(X, y)\n",
    "print(reg_cv.best_score_, reg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:54:37.544830Z",
     "start_time": "2020-12-04T04:54:33.980664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.324801148033665 {'alpha': 0.01, 'normalize': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "parameters = {'alpha': np.logspace(-2, 2, 5), 'normalize':[False, True]}\n",
    "reg = Lasso()\n",
    "reg_cv = GridSearchCV(reg, parameters, scoring='neg_mean_squared_error')\n",
    "reg_cv.fit(X, y)\n",
    "print(reg_cv.best_score_, reg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T05:03:27.106216Z",
     "start_time": "2020-12-04T05:03:01.692354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.9972299419944424\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.9563351466178758\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.937427502211689\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.946147044859739\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.870777995396883\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.9972299419944424\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.9563351466178758\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.937427502211689\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.946147044859739\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\pyann\\anaconda3\\envs\\python3.8\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: nan, tolerance: 3.870777995396883\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.4434873215122535 {'alpha': 0.1, 'l1_ratio': 0.5, 'normalize': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "parameters = {'alpha': np.logspace(-1, 1, 4),\n",
    "              'l1_ratio': np.linspace(0.5, 5, 4), \n",
    "              'normalize':[False, True]}\n",
    "reg = ElasticNet()\n",
    "reg_cv = GridSearchCV(reg, parameters, scoring='neg_mean_squared_error')\n",
    "reg_cv.fit(X, y)\n",
    "print(reg_cv.best_score_, reg_cv.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
