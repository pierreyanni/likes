{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth submission: Support Vector Regression\n",
    " * min observations in each category: 2\n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:21.432195Z",
     "start_time": "2020-12-12T20:23:20.157828Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and drop features, manage categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:22.374418Z",
     "start_time": "2020-12-12T20:23:22.172751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to prepare data\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def create_new_features(X):\n",
    "    # transform utc offset:\n",
    "    X['Sin_UTC'] = np.sin((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    X['Cos_UTC'] = np.cos((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    \n",
    "    # time since creation in days\n",
    "    duration = (pd.to_datetime('today') - \n",
    "                pd.to_datetime(X['Profile Creation Timestamp']).dt.tz_localize(None))\n",
    "    X['Duration'] = duration.apply(lambda x: x.days)\n",
    "    \n",
    "    # convert personal url into True/False (NaNs or unique)\n",
    "    X['Has Personal URL'] = X['Personal URL'].notna()\n",
    "    \n",
    "    # log(x + 1) of numerical features\n",
    "    for feature in ['Num of Followers', 'Num of People Following',\n",
    "                    'Num of Status Updates', 'Num of Direct Messages',\n",
    "                    'Avg Daily Profile Visit Duration in seconds',\n",
    "                    'Avg Daily Profile Clicks']:\n",
    "        X[f'log {feature}'] = np.log1p(X[feature])\n",
    "    return X\n",
    "    \n",
    "def clean_features(X):\n",
    "    # merge categories names with and without cap letter\n",
    "    X['Location Public Visibility'] = X['Location Public Visibility'].str.lower()\n",
    "    X.loc[X['Profile Category'] == \" \", 'Profile Category'] = 'unknown'\n",
    "    return X\n",
    "\n",
    "def drop_features(X, l_features):\n",
    "    return X.drop(columns=l_features)\n",
    "    \n",
    "class RemoveCategories(TransformerMixin):\n",
    "    def __init__(self, min_obs=10, l_cols=[]):\n",
    "        self.min_obs = min_obs\n",
    "        self.l_cats = {}\n",
    "        self.l_cols = l_cols\n",
    "\n",
    "    def fit(self, X):\n",
    "        # assumes all columns of df_cat are strings\n",
    "        for col in self.l_cols:\n",
    "            val_counts = X[col].fillna('missing').value_counts()\n",
    "            self.l_cats[col] = val_counts[val_counts >= self.min_obs].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        for col in self.l_cols:\n",
    "            X.loc[~ X[col].isin(self.l_cats[col]), col] = 'other'        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:24.544055Z",
     "start_time": "2020-12-12T20:23:24.517023Z"
    }
   },
   "outputs": [],
   "source": [
    "# data cleaning and feature engineering\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "l_cat_small = ['Profile Text Color', 'Profile Page Color',\n",
    "               'Profile Theme Color', 'User Language', 'Location']\n",
    "l_drop = ['Id', 'User Name', 'Profile Image', 'Personal URL',\n",
    "          'Profile Creation Timestamp', 'Location', 'UTC Offset']\n",
    "l_drop_logs = ['Num of Followers', 'Num of People Following',\n",
    "               'Num of Status Updates', 'Num of Direct Messages',\n",
    "               'Avg Daily Profile Visit Duration in seconds', \n",
    "               'Avg Daily Profile Clicks']\n",
    "\n",
    "features_to_drop =dict(l_features = l_drop + l_drop_logs)\n",
    "\n",
    "pipe = Pipeline([('create new features',\n",
    "                  FunctionTransformer(create_new_features)),\n",
    "                 ('clean data', FunctionTransformer(clean_features)),\n",
    "                 ('remove categories', RemoveCategories(min_obs=10, \n",
    "                                                        l_cols=l_cat_small)),\n",
    "                 ('drop variables', FunctionTransformer(drop_features,\n",
    "                                    kw_args=features_to_drop))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and encoding/scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:26.785473Z",
     "start_time": "2020-12-12T20:23:26.661286Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "# categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:31.402324Z",
     "start_time": "2020-12-12T20:23:29.924426Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "y = np.log1p(df['Num of Profile Likes'].values)\n",
    "X = df.drop(columns='Num of Profile Likes')\n",
    "\n",
    "X = pipe.fit_transform(X)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer,\n",
    "                   X.select_dtypes(exclude=['object', 'bool']).columns),\n",
    "                  ('cat', categorical_transformer,\n",
    "                   X.select_dtypes(include=['object', 'bool']).columns)])\n",
    "\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the columns names back\n",
    "\n",
    "from sklearn.feature_extraction.text import _VectorizerMixin\n",
    "from sklearn.feature_selection._base import SelectorMixin\n",
    "def get_feature_out(estimator, feature_in):\n",
    "    if hasattr(estimator,'get_feature_names'):\n",
    "        if isinstance(estimator, _VectorizerMixin):\n",
    "            # handling all vectorizers\n",
    "            return [f'vec_{f}' for f in estimator.get_feature_names()]\n",
    "        else:\n",
    "            return estimator.get_feature_names(feature_in)\n",
    "    elif isinstance(estimator, SelectorMixin):\n",
    "        return np.array(feature_in)[estimator.get_support()]\n",
    "    else:\n",
    "        return feature_in\n",
    "def get_ct_feature_names(ct):\n",
    "    # handles all estimators, pipelines inside ColumnTransfomer\n",
    "    # doesn't work when remainder =='passthrough'\n",
    "    # which requires the input column names.\n",
    "    output_features = []\n",
    "    for name, estimator, features in ct.transformers_:\n",
    "        if name!='remainder':\n",
    "            if isinstance(estimator, Pipeline):\n",
    "                current_features = features\n",
    "                for step in estimator:\n",
    "                    current_features = get_feature_out(step, current_features)\n",
    "                features_out = current_features\n",
    "            else:\n",
    "                features_out = get_feature_out(estimator, features)\n",
    "            output_features.extend(features_out)\n",
    "        elif estimator=='passthrough':\n",
    "            output_features.extend(ct._feature_names_in[features])\n",
    "    return output_features\n",
    "\n",
    "df_X = pd.DataFrame(X, columns=get_ct_feature_names(preprocessor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7500 entries, 0 to 7499\n",
      "Columns: 224 entries, Sin_UTC to Has Personal URL_True\n",
      "dtypes: float64(224)\n",
      "memory usage: 12.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/conda/pyanni/envs/kaggle_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=100)]: Using backend LokyBackend with 100 concurrent workers.\n",
      "[Parallel(n_jobs=100)]: Done   1 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=100)]: Done  11 out of 100 | elapsed:   16.0s remaining:  2.2min\n",
      "[Parallel(n_jobs=100)]: Done  22 out of 100 | elapsed:   18.6s remaining:  1.1min\n",
      "[Parallel(n_jobs=100)]: Done  33 out of 100 | elapsed:   44.7s remaining:  1.5min\n",
      "[Parallel(n_jobs=100)]: Done  44 out of 100 | elapsed:   51.3s remaining:  1.1min\n",
      "[Parallel(n_jobs=100)]: Done  55 out of 100 | elapsed:   52.6s remaining:   43.0s\n",
      "[Parallel(n_jobs=100)]: Done  66 out of 100 | elapsed:   53.1s remaining:   27.3s\n",
      "[Parallel(n_jobs=100)]: Done  77 out of 100 | elapsed:   54.3s remaining:   16.2s\n",
      "[Parallel(n_jobs=100)]: Done  88 out of 100 | elapsed:   54.9s remaining:    7.5s\n",
      "[Parallel(n_jobs=100)]: Done 100 out of 100 | elapsed:   56.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE: 1.737 \n",
      "best parameters: {'kernel': 'rbf', 'C': 1.3684210526315788}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "parameters = {'kernel': ['rbf'],\n",
    "              'C': np.linspace(0, 2, 20)}\n",
    "\n",
    "reg_svr = RandomizedSearchCV(svr, parameters, scoring='neg_mean_squared_error',\n",
    "                            n_jobs=100, random_state=None, n_iter=50, verbose=10)\n",
    "reg_svr.fit(X, y)\n",
    "\n",
    "print(f'RMSLE: {np.sqrt(-reg_svr.best_score_):.3f}',\n",
    "      f'\\nbest parameters: { reg_svr.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:07.576617Z",
     "start_time": "2020-12-12T20:23:07.569636Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_submission(estimator, number):\n",
    "    X_test = pd.read_csv('data/test.csv')\n",
    "    Id = X_test['Id']\n",
    "    X_test = pipe.transform(X_test)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    y_pred = (estimator.predict(X_test))\n",
    "    y_pred_test = pd.Series(np.expm1(y_pred))\n",
    "    submission = pd.DataFrame({'Id': Id, 'Predicted': y_pred_test})\n",
    "    path = f'submissions/submission{number}.csv'\n",
    "    while os.path.exists(path):\n",
    "        number += 1\n",
    "        path = f'submissions/submission{number}.csv'\n",
    "    submission.to_csv(path, index=False)\n",
    "    print(f'submission #{number} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:07.745663Z",
     "start_time": "2020-12-12T20:23:07.579936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission #4 created\n"
     ]
    }
   ],
   "source": [
    "create_submission(reg_svr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:07.747820Z",
     "start_time": "2020-12-12T20:23:07.573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49I3SOKLI2CMNGP4</td>\n",
       "      <td>2809.866389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>727IRIR59A3P88LK</td>\n",
       "      <td>3132.848317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LN95SD15SRPCEE8F</td>\n",
       "      <td>327.989895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TB11I7F0PN033D4T</td>\n",
       "      <td>5138.042429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32PSGCK5PATHMR07</td>\n",
       "      <td>271.653679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id    Predicted\n",
       "0  49I3SOKLI2CMNGP4  2809.866389\n",
       "1  727IRIR59A3P88LK  3132.848317\n",
       "2  LN95SD15SRPCEE8F   327.989895\n",
       "3  TB11I7F0PN033D4T  5138.042429\n",
       "4  32PSGCK5PATHMR07   271.653679"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('submissions/submission4.csv').head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
