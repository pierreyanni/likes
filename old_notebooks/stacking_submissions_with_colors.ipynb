{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:21.432195Z",
     "start_time": "2020-12-12T20:23:20.157828Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from colour.notation import HEX_to_RGB\n",
    "from colour import convert\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and drop features, manage categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:22.374418Z",
     "start_time": "2020-12-12T20:23:22.172751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to prepare data\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def create_new_features(X):\n",
    "    # transform utc offset:\n",
    "    X['Sin_UTC'] = np.sin((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    X['Cos_UTC'] = np.cos((11 * 3600 + X['UTC Offset']) / (24 * 3600) * 2 * np.pi)\n",
    "    \n",
    "    # time since creation in days\n",
    "    duration = (pd.to_datetime('today') - \n",
    "                pd.to_datetime(X['Profile Creation Timestamp']).dt.tz_localize(None))\n",
    "    X['Duration'] = duration.apply(lambda x: x.days)\n",
    "    \n",
    "    # convert personal url into True/False (NaNs or unique)\n",
    "    X['Has Personal URL'] = X['Personal URL'].notna()\n",
    "    \n",
    "    # log(x + 1) of numerical features\n",
    "    for feature in ['Num of Followers', 'Num of People Following',\n",
    "                    'Num of Status Updates', 'Num of Direct Messages',\n",
    "                    'Avg Daily Profile Visit Duration in seconds',\n",
    "                    'Avg Daily Profile Clicks']:\n",
    "        X[f'log {feature}'] = np.log1p(X[feature])\n",
    "    \n",
    "    for col in ['Profile Text Color', 'Profile Page Color', 'Profile Theme Color']:\n",
    "        X[f'Lab {col}'] = X[col].apply(lambda x: convert(HEX_to_RGB(x), 'RGB', 'CIE Lab') if not pd.isnull(x) \n",
    "                else convert((1,1,1), 'RGB', 'CIE Lab'))\n",
    "\n",
    "    return X\n",
    "    \n",
    "def clean_features(X):\n",
    "    # merge categories names with and without cap letter\n",
    "    X['Location Public Visibility'] = X['Location Public Visibility'].str.lower()\n",
    "    X.loc[X['Profile Category'] == \" \", 'Profile Category'] = 'unknown'\n",
    "    return X\n",
    "\n",
    "def drop_features(X, l_features):\n",
    "    return X.drop(columns=l_features)\n",
    "    \n",
    "class RemoveCategories(TransformerMixin):\n",
    "    def __init__(self, min_obs=10, l_cols=[]):\n",
    "        self.min_obs = min_obs\n",
    "        self.l_cats = {}\n",
    "        self.l_cols = l_cols\n",
    "\n",
    "    def fit(self, X):\n",
    "        # assumes all columns of df_cat are strings\n",
    "        for col in self.l_cols:\n",
    "            val_counts = X[col].fillna('missing').value_counts()\n",
    "            self.l_cats[col] = val_counts[val_counts >= self.min_obs].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        for col in self.l_cols:\n",
    "            X.loc[~ X[col].isin(self.l_cats[col]), col] = 'other'        \n",
    "        return X\n",
    "\n",
    "class ClusterColors(TransformerMixin):\n",
    "    \"\"\" The data in X[ccols] should be CIE Lab color data\"\"\"\n",
    "    def __init__(self, ccols=['Lab Profile Text Color', 'Lab Profile Page Color', 'Lab Profile Theme Color'], *args, **kwargs):\n",
    "        self.clusterers = dict(zip(ccols, [KMeans(*args, **kwargs)]*len(ccols)))\n",
    "        self.columns = ccols\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for clusterer in self.clusterers.values():\n",
    "            clusterer.set_params(**params)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):        \n",
    "        for col in self.columns:\n",
    "            self.clusterers[col].fit(list(X[col].to_numpy()))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for col in self.columns:\n",
    "            X[f'Clustered {col}'] = self.clusterers[col].predict(list(X[col].to_numpy()))\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:24.544055Z",
     "start_time": "2020-12-12T20:23:24.517023Z"
    }
   },
   "outputs": [],
   "source": [
    "# data cleaning and feature engineering\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "l_cat_small = ['User Language', 'Location']\n",
    "l_drop = ['Id', 'User Name', 'Profile Image', 'Personal URL',\n",
    "          'Profile Creation Timestamp', 'Location', 'UTC Offset']\n",
    "l_drop_logs = ['Num of Followers', 'Num of People Following',\n",
    "               'Num of Status Updates', 'Num of Direct Messages',\n",
    "               'Avg Daily Profile Visit Duration in seconds', \n",
    "               'Avg Daily Profile Clicks']\n",
    "l_drop_hex = ['Profile Text Color', 'Profile Page Color',\n",
    "               'Profile Theme Color'\n",
    "               ]\n",
    "features_to_drop =dict(l_features = l_drop + l_drop_logs + l_drop_hex)\n",
    "\n",
    "pipe = Pipeline([('create new features',\n",
    "                  FunctionTransformer(create_new_features)),\n",
    "                 ('clean data', FunctionTransformer(clean_features)),\n",
    "                 ('cluster', ClusterColors(n_clusters=7, n_init=10, max_iter=300, tol=1e-4)),\n",
    "                 ('remove categories', RemoveCategories(min_obs=2, \n",
    "                                                        l_cols=l_cat_small)),\n",
    "                 ('drop variables', FunctionTransformer(drop_features,\n",
    "                                    kw_args=features_to_drop))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and encoding/scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:26.785473Z",
     "start_time": "2020-12-12T20:23:26.661286Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "# categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T20:23:31.402324Z",
     "start_time": "2020-12-12T20:23:29.924426Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "y = np.log1p(df['Num of Profile Likes'].values)\n",
    "X = df.drop(columns='Num of Profile Likes')\n",
    "\n",
    "X = pipe.fit_transform(X)\n",
    "\n",
    "ccols = ['Lab Profile Text Color', 'Lab Profile Page Color', 'Lab Profile Theme Color',]\n",
    "catcols = list(X.select_dtypes(include=['object', 'bool']).columns) + ['Clustered '+c for c in ccols]\n",
    "cat_features = list([col for col in catcols if col not in ccols])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer,\n",
    "                   X.select_dtypes(exclude=['object', 'bool']).columns),\n",
    "                  ('cat', categorical_transformer,\n",
    "                   cat_features)])\n",
    "\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(estimator, number):\n",
    "    X_test = pd.read_csv('data/test.csv')\n",
    "    Id = X_test['Id']\n",
    "    X_test = pipe.transform(X_test)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    y_pred = (estimator.predict(X_test))\n",
    "    y_pred_test = pd.Series(np.expm1(y_pred))\n",
    "    submission = pd.DataFrame({'Id': Id, 'Predicted': y_pred_test})\n",
    "    path = f'submissions/submission{number}.csv'\n",
    "    while os.path.exists(path):\n",
    "        number += 1\n",
    "        path = f'submissions/submission{number}.csv'\n",
    "    submission.to_csv(path, index=False)\n",
    "    print(f'submission #{number} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "ridge = Ridge(alpha=36.7, normalize=False)\n",
    "rf = RandomForestRegressor(n_estimators=700, min_samples_split=3, min_samples_leaf=3, max_depth=18)\n",
    "gb = GradientBoostingRegressor(n_iter_no_change=6, n_estimators=10000,\n",
    "                                     min_samples_split=21, min_samples_leaf=9,\n",
    "                                     max_depth=5, learning_rate=0.06)\n",
    "bagged_svr =  BaggingRegressor(base_estimator=SVR(kernel='rbf', C=1.37))\n",
    "\n",
    "estimators = [('ridge', ridge), ('rf', rf), ('gb', gb), ('bagged_svr', bagged_svr)]\n",
    "stack = StackingRegressor(estimators=estimators, final_estimator=RidgeCV(), verbose=10, n_jobs=100, passthrough=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'ridge__alpha': np.logspace(-2,2,10),\n",
    "    'rf__n_estimators': range(400,1001,100),\n",
    "    'rf__max_depth': range(10,30,5),\n",
    "    'gb__learning_rate': np.linspace(0.1,0.5,5),\n",
    "    'bagged_svr__base_estimator__C': np.logspace(-2,2,5),\n",
    "    'final_estimator__alphas': np.logspace(-2,2,5),\n",
    "    'final_estimator__cv': [ShuffleSplit(n_splits=5, test_size=0.3, random_state=0),],\n",
    "    }\n",
    "\n",
    "stacked_cv = RandomizedSearchCV(estimator=stack, param_grid=parameters, scoring='neg_mean_squared_error', n_iter=100, n_jobs=-1)\n",
    "stacked_cv.fit(X, y)\n",
    "print(np.sqrt(-stacked_cv.best_score_), stacked_cv.best_params_)"
   ]
  },
  {
   "source": [
    " ## Make submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = stacked_cv.best_estimator_\n",
    "create_submission(stack, 13)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('arxiv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b0d8ce7e747a98a602fa47e9fc07d4d567975d4b793766e185ebf23c03a7334f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}